"""
Model implementation for MicroGPT.

This module contains the autograd Value class, parameter initialization,
and the forward pass for a minimal GPT-style model.
"""

import math
import random
from typing import Dict, List, Tuple

# --- Autograd scalar -------------------------------------------------------


class Value:
    __slots__ = ("data", "grad", "_children", "_local_grads")

    def __init__(self, data, children=(), local_grads=()):
        self.data = data
        self.grad = 0
        self._children = children
        self._local_grads = local_grads

    def __add__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data + other.data, (self, other), (1, 1))

    def __mul__(self, other):
        other = other if isinstance(other, Value) else Value(other)
        return Value(self.data * other.data, (self, other), (other.data, self.data))

    def __pow__(self, other):
        return Value(self.data**other, (self,), (other * self.data ** (other - 1),))

    def log(self):
        return Value(math.log(self.data), (self,), (1 / self.data,))

    def exp(self):
        return Value(math.exp(self.data), (self,), (math.exp(self.data),))

    def relu(self):
        return Value(max(0, self.data), (self,), (float(self.data > 0),))

    def __neg__(self):
        return self * -1

    def __radd__(self, other):
        return self + other

    def __sub__(self, other):
        return self + (-other)

    def __rsub__(self, other):
        return other + (-self)

    def __rmul__(self, other):
        return self * other

    def __truediv__(self, other):
        return self * other**-1

    def __rtruediv__(self, other):
        return other * self**-1

    def backward(self):
        topo = []
        visited = set()

        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._children:
                    build_topo(child)
                topo.append(v)

        build_topo(self)
        self.grad = 1
        for v in reversed(topo):
            for child, local_grad in zip(v._children, v._local_grads):
                child.grad += local_grad * v.grad


# --- Parameters ------------------------------------------------------------


def matrix(nout: int, nin: int, std: float = 0.08) -> List[List[Value]]:
    return [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]


def init_state_dict(
    vocab_size: int,
    n_embd: int,
    n_head: int,
    n_layer: int,
    block_size: int,
) -> Dict[str, List[List[Value]]]:
    if n_embd % n_head != 0:
        raise ValueError("n_embd must be divisible by n_head")

    state_dict: Dict[str, List[List[Value]]] = {
        "wte": matrix(vocab_size, n_embd),
        "wpe": matrix(block_size, n_embd),
        "lm_head": matrix(vocab_size, n_embd),
    }
    for i in range(n_layer):
        state_dict[f"layer{i}.attn_wq"] = matrix(n_embd, n_embd)
        state_dict[f"layer{i}.attn_wk"] = matrix(n_embd, n_embd)
        state_dict[f"layer{i}.attn_wv"] = matrix(n_embd, n_embd)
        state_dict[f"layer{i}.attn_wo"] = matrix(n_embd, n_embd)
        state_dict[f"layer{i}.mlp_fc1"] = matrix(4 * n_embd, n_embd)
        state_dict[f"layer{i}.mlp_fc2"] = matrix(n_embd, 4 * n_embd)
    return state_dict


def flatten_params(state_dict: Dict[str, List[List[Value]]]) -> List[Value]:
    return [p for mat in state_dict.values() for row in mat for p in row]


def state_dict_to_floats(
    state_dict: Dict[str, List[List[Value]]],
) -> Dict[str, List[List[float]]]:
    return {
        name: [[v.data for v in row] for row in mat] for name, mat in state_dict.items()
    }


def state_dict_from_floats(
    state_dict_floats: Dict[str, List[List[float]]],
) -> Dict[str, List[List[Value]]]:
    return {
        name: [[Value(v) for v in row] for row in mat]
        for name, mat in state_dict_floats.items()
    }


# --- Model forward ---------------------------------------------------------


def linear(x: List[Value], w: List[List[Value]]) -> List[Value]:
    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]


def softmax(logits: List[Value]) -> List[Value]:
    max_val = max(val.data for val in logits)
    exps = [(val - max_val).exp() for val in logits]
    total = sum(exps)
    return [e / total for e in exps]


def rmsnorm(x: List[Value]) -> List[Value]:
    ms = sum(xi * xi for xi in x) / len(x)
    scale = (ms + 1e-5) ** -0.5
    return [xi * scale for xi in x]


def gpt_forward(
    token_id: int,
    pos_id: int,
    keys: List[List[List[Value]]],
    values: List[List[List[Value]]],
    state_dict: Dict[str, List[List[Value]]],
    n_layer: int,
    n_head: int,
    n_embd: int,
) -> List[Value]:
    head_dim = n_embd // n_head
    tok_emb = state_dict["wte"][token_id]
    pos_emb = state_dict["wpe"][pos_id]
    x = [t + p for t, p in zip(tok_emb, pos_emb)]
    x = rmsnorm(x)

    for li in range(n_layer):
        # 1) Multi-head attention
        x_residual = x
        x = rmsnorm(x)
        q = linear(x, state_dict[f"layer{li}.attn_wq"])
        k = linear(x, state_dict[f"layer{li}.attn_wk"])
        v = linear(x, state_dict[f"layer{li}.attn_wv"])
        keys[li].append(k)
        values[li].append(v)

        x_attn: List[Value] = []
        for h in range(n_head):
            hs = h * head_dim
            q_h = q[hs : hs + head_dim]
            k_h = [ki[hs : hs + head_dim] for ki in keys[li]]
            v_h = [vi[hs : hs + head_dim] for vi in values[li]]
            attn_logits = [
                sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5
                for t in range(len(k_h))
            ]
            attn_weights = softmax(attn_logits)
            head_out = [
                sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h)))
                for j in range(head_dim)
            ]
            x_attn.extend(head_out)

        x = linear(x_attn, state_dict[f"layer{li}.attn_wo"])
        x = [a + b for a, b in zip(x, x_residual)]

        # 2) MLP
        x_residual = x
        x = rmsnorm(x)
        x = linear(x, state_dict[f"layer{li}.mlp_fc1"])
        x = [xi.relu() for xi in x]
        x = linear(x, state_dict[f"layer{li}.mlp_fc2"])
        x = [a + b for a, b in zip(x, x_residual)]

    logits = linear(x, state_dict["lm_head"])
    return logits
